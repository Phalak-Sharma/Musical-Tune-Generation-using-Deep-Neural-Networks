# -*- coding: utf-8 -*-
"""finalcode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1op4tSkhgM4g7P9mAoyMvTM3FYSVFm-7g
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

#library for understanding music
from music21 import *

import random
from keras.layers import *
from keras.models import *
from keras.callbacks import *
import keras.backend as K
from keras.models import load_model

import os
#Array Processing
import numpy as np

from collections import Counter
#library for visualiation
import matplotlib.pyplot as plt

# function to parse MIDI files

def read_midiFile(file, fileName):
    
    print(fileName)
    notes_list=[]
    notes_list_to_parse = None
    
    #parsing a midi file
    midiFile = converter.parse(file)
  
    #grouping based on different instruments
    pianoGroup = instrument.partitionByInstrument(midiFile)

    #Looping over all the instruments
    for instr in pianoGroup.parts:
    
        #select elements of only piano
        if 'Piano' in str(instr): 
        
            notes_list_to_parse = instr.recurse() 
      
            #finding whether a particular element is note or a chord
            for element in notes_list_to_parse:
                
                #note
                if isinstance(element, note.Note):
                    notes_list.append(str(element.pitch))
                
                #chord
                elif isinstance(element, chord.Chord):
                    notes_list.append('.'.join(str(n) for n in element.normalOrder))

    return np.array(notes_list)

#loading the dataset
#specifying the path
path="/content/drive/My Drive/Minor /dataset/"

#read all the filenames
filenames=[i for i in os.listdir(path) if i.endswith(".mid")]
print("Loading Music Files: ")
#reading MIDI files
notes_list_array = np.array([read_midiFile((path+i), i) for i in filenames])

print(notes_list_array)

#converting 2D array into 1D array
notes_list_ = [element for note_ in notes_list_array for element in note_]

#No. of unique notes
unique_notes_list = list(set(notes_list_))
print(len(unique_notes_list))

#computing frequency of each note
frequency = dict(Counter(notes_list_))
print(frequency)

#consider only the frequencies
no=[count for _,count in frequency.items()]

#set the figure size
plt.figure(figsize=(5,5))
plt.xlabel("frequency range")
plt.ylabel("no. of notes")
#plot
plt.hist(no)

#defining threshold for extracting frequent notes
frequent_notes_list = [note for note, count in frequency.items() if count>=40]
print(len(frequent_notes_list))

#selecting notes from MIDI file which satisfy the threshold
generated_music=[]

for notes_list in notes_list_array:
    temp=[]
    for note_ in notes_list:
        if note_ in frequent_notes_list:
            temp.append(note_)            
    generated_music.append(temp)
    
generated_music = np.array(generated_music)

print(generated_music)

#defining timesteps to split each MIDI file into sequences
num_timesteps = 64 
x = []
y = []

for note_ in generated_music:
    for i in range(0, len(note_) - num_timesteps, 1):
        
        #preparing input and output sequences
        input_ = note_[i:i + num_timesteps] # 0 to 63
        output = note_[i + num_timesteps] # 64
        
        x.append(input_)
        y.append(output)
        
x=np.array(x)
y=np.array(y)

print(x)

print(y)

#identifying unique notes in input
unique_x = list(set(x.ravel()))
#encoding unique input notes
x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))

#preparing input sequences
x_seq=[]
for i in x:
    temp=[]
    for j in i:
        #assigning unique integer to every note
        temp.append(x_note_to_int[j])
    x_seq.append(temp)
    
x_seq = np.array(x_seq)

print(len(x_seq))
print(x_seq)

#identifying unique notes in output
unique_y = list(set(y))
#encoding unique output notes
y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) 
y_seq=np.array([y_note_to_int[i] for i in y])

print(len(y_seq))
print(y_seq)

from sklearn.model_selection import train_test_split
x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)

"""WAVENET"""

K.clear_session()
model = Sequential()
    
#embedding layer
model.add(Embedding(len(unique_x), 100, input_length=64,trainable=True)) 

model.add(Conv1D(64,3, padding='causal',activation='relu'))
model.add(Dropout(0.2))
model.add(MaxPool1D(2))
    
model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))
model.add(Dropout(0.2))
model.add(MaxPool1D(2))

model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))
model.add(Dropout(0.2))
model.add(MaxPool1D(2))
          
#model.add(Conv1D(256,5,activation='relu'))    
model.add(GlobalMaxPool1D())
    
model.add(Dense(256, activation='relu'))
model.add(Dense(len(unique_y), activation='softmax'))
    
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])

model.summary()

mc=ModelCheckpoint('best_modelkeras.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)

history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])

#loading best model
model = load_model('best_modelkeras.h5')

def plot_graph_loss(history):
  for label in ["loss"]:
    plt.plot(history.history[label],label=label)
  for label in ["val_loss"]:
    plt.plot(history.history[label],label=label)
  plt.legend()
  plt.xlabel("epochs")
  plt.ylabel("loss")
  plt.show()

def plot_graph_acc(history):
  for label in ["accuracy"]:
    plt.plot(history.history[label],label=label)
  for label in ["val_accuracy"]:
    plt.plot(history.history[label],label=label)
  plt.legend()
  plt.xlabel("epochs")
  plt.ylabel("accuracy")
  plt.show()

plot_graph_loss(history)

plot_graph_acc(history)

val_loss=history.history['val_loss']
accuracy=history.history['accuracy']
loss=history.history['loss']
val_accuracy=history.history['val_accuracy']

val_loss_min=min(val_loss)
epoch=val_loss.index(val_loss_min)
acc=accuracy[epoch]
train_loss=loss[epoch]
val_acc=val_accuracy[epoch]

print("Training Set Accuracy: ", acc*100)
print("Training Set Loss: ", train_loss)
print("Validation Set Accuracy: ", val_acc*100)
print("Validation Set Loss: ", val_loss_min)

#predicting new music notes
index = np.random.randint(0,len(x_val)-1)
#selecting a input sequence from validation set
output_music = x_val[index]

predictions=[]
for i in range(20):

    output_music = output_music.reshape(1,num_timesteps)

    probability  = model.predict(output_music)[0]
    y_prediction= np.argmax(probability,axis=0)
    predictions.append(y_prediction)

    output_music = np.insert(output_music[0],len(output_music[0]),y_prediction)
    output_music = output_music[1:]
    
print(predictions)

#converting integer predictions to corresponding notes
x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) 
predicted_notes_list = [x_int_to_note[i] for i in predictions]

#creating midi file for the predicted notes
def convert_to_midiFile(prediction_output):
   
    offset = 0
    output_notes_list = []

    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_list_in_chord = pattern.split('.')
            notes_list = []
            for cur_note in notes_list_in_chord:
                
                cn=int(cur_note)
                new_note = note.Note(cn)
                new_note.storedInstrument = instrument.Piano()
                notes_list.append(new_note )
                
            new_chord = chord.Chord(notes_list)
            new_chord.offset = offset
            output_notes_list.append(new_chord)
            
        # pattern is a note
        else:
            
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes_list.append(new_note)

        # increase offset each iteration so that notes do not stack
        offset += 1
    
    midiFile_stream = stream.Stream(output_notes_list)
    midiFile_stream.write('midi', fp='thresh50time32wavenet.mid')

convert_to_midiFile(predicted_notes_list)

"""LSTM"""

n_vocabulary=len(unique_y)
model1 = Sequential()
model1.add(Embedding(len(unique_x), 100, input_length=64,trainable=True)) 
model1.add(LSTM(128,return_sequences=True))
model1.add(LSTM(128))
model1.add(Dense(256))
model1.add(Activation('relu'))
model1.add(Dense(n_vocabulary))
model1.add(Activation('softmax'))
model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])

model1.summary()

mc1=ModelCheckpoint('best_modellstm.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)

history1 = model1.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=25, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc1])

model1 = load_model('best_modellstm.h5')

plot_graph_loss(history1)

plot_graph_acc(history1)

val_loss1=history1.history['val_loss']
accuracy1=history1.history['accuracy']
loss1=history1.history['loss']
val_accuracy1=history1.history['val_accuracy']

val_loss_min1=min(val_loss1)
epoch1=val_loss1.index(val_loss_min1) 
acc1=accuracy1[epoch1]
train_loss1=loss1[epoch1]
val_acc1=val_accuracy1[epoch1]

print("Training Set Accuracy: ", acc1*100)
print("Training Set Loss: ", train_loss1)
print("Validation Set Accuracy: ", val_acc1*100)
print("Validation Set Loss: ", val_loss_min1)

#predicting new music notes
index1 = np.random.randint(0,len(x_val)-1)
#selecting a input sequence from validation set
output_music1 = x_val[index1]

predictions1=[]
for i in range(20):

    output_music1 = output_music1.reshape(1,num_timesteps)

    probability1  = model1.predict(output_music1)[0]
    y_prediction1 = np.argmax(probability1,axis=0)
    predictions1.append(y_prediction1)

    output_music1 = np.insert(output_music1[0],len(output_music1[0]),y_prediction1)
    output_music1 = output_music1[1:]
    
print(predictions1)

#converting integer predictions to corresponding notes
x_int_to_note1 = dict((number, note_) for number, note_ in enumerate(unique_x)) 
predicted_notes_list1 = [x_int_to_note1[i] for i in predictions1]

#creating midi file for the predicted notes
def convert_to_midiFile1(prediction_output):
   
    offset = 0
    output_notes_list = []

    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:

        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_list_in_chord = pattern.split('.')
            notes_list = []
            for cur_note in notes_list_in_chord:
                
                cn=int(cur_note)
                new_note = note.Note(cn)
                new_note.storedInstrument = instrument.Piano()
                notes_list.append(new_note)
                
            new_chord = chord.Chord(notes_list)
            new_chord.offset = offset
            output_notes_list.append(new_chord)

        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes_list.append(new_note)

        # increase offset each iteration so that notes do not stack
        offset += 1
        
    midiFile_stream = stream.Stream(output_notes_list)
    midiFile_stream.write('midi', fp='8thresh40time64LSTM.mid')

convert_to_midiFile1(predicted_notes_list1)

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# jupyter nbconvert --to html /content/finalcode.ipynb

